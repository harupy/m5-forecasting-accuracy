{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This kernel is:\n",
    "- Based on [Very fst Model](https://www.kaggle.com/ragnar123/very-fst-model). Thanks @ragnar!\n",
    "- Automatically uploaded by [push-kaggle-kernel](https://github.com/harupy/push-kaggle-kernel).\n",
    "- Formatted by [Black](https://github.com/psf/black)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective\n",
    "\n",
    "* Make a baseline model that predict the validation (28 days).\n",
    "* This competition has 2 stages, so the main objective is to make a model that can predict the demand for the next 28 days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn import preprocessing, metrics\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option(\"display.max_columns\", 500)\n",
    "pd.set_option(\"display.max_rows\", 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "def on_kaggle():\n",
    "    return \"KAGGLE_KERNEL_RUN_TYPE\" in os.environ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1,
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "if on_kaggle():\n",
    "    os.system(\"git clone https://github.com/harupy/m5-forecasting-accuracy\")\n",
    "    os.system(\"pip install mlflow_extend\")\n",
    "    sys.path.append(\"m5-forecasting-accuracy/src\")\n",
    "else:\n",
    "    sys.path.append(\"src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = [\"int16\", \"int32\", \"int64\", \"float16\", \"float32\", \"float64\"]\n",
    "    start_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == \"int\":\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if (\n",
    "                    c_min > np.finfo(np.float16).min\n",
    "                    and c_max < np.finfo(np.float16).max\n",
    "                ):\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif (\n",
    "                    c_min > np.finfo(np.float32).min\n",
    "                    and c_max < np.finfo(np.float32).max\n",
    "                ):\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    end_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    if verbose:\n",
    "        print(\n",
    "            \"Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)\".format(\n",
    "                end_mem, 100 * (start_mem - end_mem) / start_mem\n",
    "            )\n",
    "        )\n",
    "    return df\n",
    "\n",
    "\n",
    "# function to read the data and merge it (ignoring some columns, this is a very fst model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2,
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    input_dir = \"/kaggle/input\" if on_kaggle() else \"input\"\n",
    "    input_dir = f\"{input_dir}/m5-forecasting-accuracy\"\n",
    "\n",
    "    print(\"Reading files...\")\n",
    "\n",
    "    calendar = pd.read_csv(f\"{input_dir}/calendar.csv\").pipe(reduce_mem_usage)\n",
    "    sell_prices = pd.read_csv(f\"{input_dir}/sell_prices.csv\").pipe(reduce_mem_usage)\n",
    "    sales_train_val = pd.read_csv(f\"{input_dir}/sales_train_validation.csv\").pipe(\n",
    "        reduce_mem_usage\n",
    "    )\n",
    "    submission = pd.read_csv(f\"{input_dir}/sample_submission.csv\").pipe(\n",
    "        reduce_mem_usage\n",
    "    )\n",
    "\n",
    "    print(\"Calendar shape:\", calendar.shape)\n",
    "    print(\"Sell prices shape:\", sell_prices.shape)\n",
    "    print(\"Sales train shape:\", sales_train_val.shape)\n",
    "    print(\"Submission shape:\", submission.shape)\n",
    "\n",
    "    return calendar, sell_prices, sales_train_val, submission\n",
    "\n",
    "\n",
    "def melt_and_merge(\n",
    "    calendar, sell_prices, sales_train_val, submission, nrows=55_000_000, merge=False,\n",
    "):\n",
    "    # melt sales data, get it ready for training\n",
    "    id_columns = [\"id\", \"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\"]\n",
    "    sales_train_val = pd.melt(\n",
    "        sales_train_val, id_vars=id_columns, var_name=\"day\", value_name=\"demand\",\n",
    "    )\n",
    "    sales_train_val = reduce_mem_usage(sales_train_val)\n",
    "\n",
    "    # separate test dataframes\n",
    "    test1 = submission[submission[\"id\"].str.contains(\"validation\")]\n",
    "    test2 = submission[submission[\"id\"].str.contains(\"evaluation\")]\n",
    "\n",
    "    # change column names\n",
    "    test1.columns = [\"id\"] + [f\"d_{x}\".format(x) for x in range(1914, 1914 + 28)]\n",
    "    test2.columns = [\"id\"] + [f\"d_{x}\".format(x) for x in range(1942, 1942 + 28)]\n",
    "\n",
    "    # get product table\n",
    "    product = sales_train_val[id_columns].drop_duplicates()\n",
    "\n",
    "    # merge with product table\n",
    "    test2[\"id\"] = test2[\"id\"].str.replace(\"_evaluation\", \"_validation\")\n",
    "    test1 = test1.merge(product, how=\"left\", on=\"id\")\n",
    "    test2 = test2.merge(product, how=\"left\", on=\"id\")\n",
    "    test2[\"id\"] = test2[\"id\"].str.replace(\"_validation\", \"_evaluation\")\n",
    "\n",
    "    test1 = pd.melt(test1, id_vars=id_columns, var_name=\"day\", value_name=\"demand\",)\n",
    "    test2 = pd.melt(test2, id_vars=id_columns, var_name=\"day\", value_name=\"demand\",)\n",
    "\n",
    "    sales_train_val[\"part\"] = \"train\"\n",
    "    test1[\"part\"] = \"test1\"\n",
    "    test2[\"part\"] = \"test2\"\n",
    "\n",
    "    data = pd.concat([sales_train_val, test1, test2], axis=0)\n",
    "\n",
    "    del sales_train_val, test1, test2\n",
    "\n",
    "    # get only a sample for fast training\n",
    "    data = data.loc[nrows:]\n",
    "\n",
    "    # drop some calendar features\n",
    "    calendar = calendar.drop([\"weekday\", \"wday\", \"month\", \"year\"], axis=1)\n",
    "\n",
    "    # delete test2 for now\n",
    "    data = data[data[\"part\"] != \"test2\"]\n",
    "\n",
    "    if merge:\n",
    "        # notebook crashes with the entire dataset\n",
    "        data = pd.merge(data, calendar, how=\"left\", left_on=[\"day\"], right_on=[\"d\"])\n",
    "        data = data.drop([\"d\", \"day\"], axis=1)\n",
    "        # get the sell price data (this feature should be very important)\n",
    "        data = data.merge(\n",
    "            sell_prices, on=[\"store_id\", \"item_id\", \"wm_yr_wk\"], how=\"left\"\n",
    "        )\n",
    "    gc.collect()\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1,
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "calendar, sell_prices, sales_train_val, submission = read_data()\n",
    "data = melt_and_merge(\n",
    "    calendar, sell_prices, sales_train_val, submission, nrows=27_500_000, merge=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We have the data to build our first model, let's build a baseline and predict the validation data (in our case is test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "def transform(data):\n",
    "    nan_features = [\"event_name_1\", \"event_type_1\", \"event_name_2\", \"event_type_2\"]\n",
    "    for feature in nan_features:\n",
    "        data[feature] = data[feature].fillna(\"MISSING\")\n",
    "\n",
    "    cat_cols = [\n",
    "        \"item_id\",\n",
    "        \"dept_id\",\n",
    "        \"cat_id\",\n",
    "        \"store_id\",\n",
    "        \"state_id\",\n",
    "        \"event_name_1\",\n",
    "        \"event_type_1\",\n",
    "        \"event_name_2\",\n",
    "        \"event_type_2\",\n",
    "    ]\n",
    "    for col in cat_cols:\n",
    "        encoder = preprocessing.LabelEncoder()\n",
    "        data[col] = encoder.fit_transform(data[col])\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def feature_engineering(data):\n",
    "    # rolling demand features\n",
    "    for shift in [28, 29, 30]:\n",
    "        data[f\"lag_t{shift}\"] = data.groupby([\"id\"])[\"demand\"].transform(\n",
    "            lambda x: x.shift(shift)\n",
    "        )\n",
    "\n",
    "    for size in [7, 30]:\n",
    "        data[f\"rolling_std_t{size}\"] = data.groupby([\"id\"])[\"demand\"].transform(\n",
    "            lambda x: x.shift(28).rolling(size).std()\n",
    "        )\n",
    "\n",
    "    for size in [7, 30, 90, 180]:\n",
    "        data[f\"rolling_mean_t{size}\"] = data.groupby([\"id\"])[\"demand\"].transform(\n",
    "            lambda x: x.shift(28).rolling(size).mean()\n",
    "        )\n",
    "\n",
    "    data[\"rolling_skew_t30\"] = data.groupby([\"id\"])[\"demand\"].transform(\n",
    "        lambda x: x.shift(28).rolling(30).skew()\n",
    "    )\n",
    "    data[\"rolling_kurt_t30\"] = data.groupby([\"id\"])[\"demand\"].transform(\n",
    "        lambda x: x.shift(28).rolling(30).kurt()\n",
    "    )\n",
    "\n",
    "    # price features\n",
    "    data[\"lag_price_t1\"] = data.groupby([\"id\"])[\"sell_price\"].transform(\n",
    "        lambda x: x.shift(1)\n",
    "    )\n",
    "    data[\"price_change_t1\"] = (data[\"lag_price_t1\"] - data[\"sell_price\"]) / (\n",
    "        data[\"lag_price_t1\"]\n",
    "    )\n",
    "    data[\"rolling_price_max_t365\"] = data.groupby([\"id\"])[\"sell_price\"].transform(\n",
    "        lambda x: x.shift(1).rolling(365).max()\n",
    "    )\n",
    "    data[\"price_change_t365\"] = (\n",
    "        data[\"rolling_price_max_t365\"] - data[\"sell_price\"]\n",
    "    ) / (data[\"rolling_price_max_t365\"])\n",
    "\n",
    "    data[\"rolling_price_std_t7\"] = data.groupby([\"id\"])[\"sell_price\"].transform(\n",
    "        lambda x: x.rolling(7).std()\n",
    "    )\n",
    "    data[\"rolling_price_std_t30\"] = data.groupby([\"id\"])[\"sell_price\"].transform(\n",
    "        lambda x: x.rolling(30).std()\n",
    "    )\n",
    "    data = data.drop([\"rolling_price_max_t365\", \"lag_price_t1\"], axis=1)\n",
    "\n",
    "    # time features\n",
    "    data[\"date\"] = pd.to_datetime(data[\"date\"])\n",
    "    data[\"year\"] = data[\"date\"].dt.year\n",
    "    data[\"month\"] = data[\"date\"].dt.month\n",
    "    data[\"week\"] = data[\"date\"].dt.week\n",
    "    data[\"day\"] = data[\"date\"].dt.day\n",
    "    data[\"dayofweek\"] = data[\"date\"].dt.dayofweek\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def run_lgb(data):\n",
    "    features = [\n",
    "        \"item_id\",\n",
    "        \"dept_id\",\n",
    "        \"cat_id\",\n",
    "        \"store_id\",\n",
    "        \"state_id\",\n",
    "        \"event_name_1\",\n",
    "        \"event_type_1\",\n",
    "        \"event_name_2\",\n",
    "        \"event_type_2\",\n",
    "        \"snap_CA\",\n",
    "        \"snap_TX\",\n",
    "        \"snap_WI\",\n",
    "        \"sell_price\",\n",
    "        \"lag_t28\",\n",
    "        \"lag_t29\",\n",
    "        \"lag_t30\",\n",
    "        \"rolling_mean_t7\",\n",
    "        \"rolling_std_t7\",\n",
    "        \"rolling_mean_t30\",\n",
    "        \"rolling_mean_t90\",\n",
    "        \"rolling_mean_t180\",\n",
    "        \"rolling_std_t30\",\n",
    "        \"price_change_t1\",\n",
    "        \"price_change_t365\",\n",
    "        \"rolling_price_std_t7\",\n",
    "        \"rolling_price_std_t30\",\n",
    "        \"rolling_skew_t30\",\n",
    "        \"rolling_kurt_t30\",\n",
    "        \"year\",\n",
    "        \"month\",\n",
    "        \"week\",\n",
    "        \"day\",\n",
    "        \"dayofweek\",\n",
    "    ]\n",
    "\n",
    "    # going to evaluate with the last 28 days\n",
    "    mask1 = data[\"date\"] <= \"2016-03-27\"\n",
    "    mask2 = data[\"date\"] <= \"2016-04-24\"\n",
    "    X_train = data[mask1]\n",
    "    y_train = X_train.pop(\"demand\")\n",
    "    X_val = data[~mask1 & mask2]\n",
    "    y_val = X_val.pop(\"demand\")\n",
    "    X_test = data[~mask2].drop(\"demand\", axis=1)\n",
    "    del data\n",
    "    gc.collect()\n",
    "\n",
    "    # define random hyperparammeters\n",
    "    params = {\n",
    "        \"boosting_type\": \"gbdt\",\n",
    "        \"metric\": \"rmse\",\n",
    "        \"objective\": \"regression\",\n",
    "        \"n_jobs\": -1,\n",
    "        \"seed\": 42,\n",
    "        \"learning_rate\": 0.1,\n",
    "        \"bagging_fraction\": 0.75,\n",
    "        \"bagging_freq\": 10,\n",
    "        \"colsample_bytree\": 0.75,\n",
    "    }\n",
    "\n",
    "    fit_params = {\n",
    "        \"num_boost_round\": 100_000,\n",
    "        \"early_stopping_rounds\": 50,\n",
    "        \"verbose_eval\": 100,\n",
    "    }\n",
    "\n",
    "    train_set = lgb.Dataset(X_train[features], y_train)\n",
    "    val_set = lgb.Dataset(X_val[features], y_val)\n",
    "\n",
    "    del X_train, y_train\n",
    "\n",
    "    model = lgb.train(params, train_set, valid_sets=[train_set, val_set], **fit_params)\n",
    "\n",
    "    val_pred = model.predict(X_val[features])\n",
    "    val_rmse = np.sqrt(metrics.mean_squared_error(val_pred, y_val))\n",
    "    print(f\"RMSE:\", val_rmse)\n",
    "    y_pred = model.predict(X_test[features])\n",
    "    return model, X_test.assign(demand=y_pred)\n",
    "\n",
    "\n",
    "def make_submission(test, submission):\n",
    "    preds = test[[\"id\", \"date\", \"demand\"]]\n",
    "    preds = pd.pivot(preds, index=\"id\", columns=\"date\", values=\"demand\").reset_index()\n",
    "    F_cols = [\"F\" + str(i + 1) for i in range(28)]\n",
    "    preds.columns = [\"id\"] + F_cols\n",
    "\n",
    "    assert preds[F_cols].isnull().sum().sum() == 0\n",
    "\n",
    "    evals = submission[submission[\"id\"].str.contains(\"evaluation\")]\n",
    "    vals = submission[[\"id\"]].merge(preds, on=\"id\")\n",
    "    final = pd.concat([vals, evals])\n",
    "    final.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "data = transform(data)\n",
    "data = feature_engineering(data)\n",
    "data = reduce_mem_usage(data)\n",
    "model, test = run_lgb(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "from mlflow_extend import plotting as mplt\n",
    "\n",
    "imp_type = \"gain\"\n",
    "features = model.feature_name()\n",
    "importances = model.feature_importance(imp_type)\n",
    "_ = mplt.feature_importance(features, importances, imp_type, limit=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "make_submission(test, submission)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "title,-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
