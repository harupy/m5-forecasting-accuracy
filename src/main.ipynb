{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This kernel is:\n",
    "- Based on [Very fst Model](https://www.kaggle.com/ragnar123/very-fst-model). Thanks @ragnar123.\n",
    "- Automatically uploaded by [push-kaggle-kernel](https://github.com/harupy/push-kaggle-kernel).\n",
    "- Formatted by [Black](https://github.com/psf/black)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective\n",
    "\n",
    "* Make a baseline model that predict the validation (28 days).\n",
    "* This competition has 2 stages, so the main objective is to make a model that can predict the demand for the next 28 days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option(\"display.max_columns\", 500)\n",
    "pd.set_option(\"display.max_rows\", 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "import IPython\n",
    "\n",
    "\n",
    "def display(*dfs, head=True):\n",
    "    for df in dfs:\n",
    "        IPython.display.display(df.head() if head else df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "def on_kaggle():\n",
    "    return \"KAGGLE_KERNEL_RUN_TYPE\" in os.environ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "if on_kaggle():\n",
    "    os.system(\"pip install --quiet mlflow_extend\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = [\"int16\", \"int32\", \"int64\", \"float16\", \"float32\", \"float64\"]\n",
    "    start_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == \"int\":\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if (\n",
    "                    c_min > np.finfo(np.float16).min\n",
    "                    and c_max < np.finfo(np.float16).max\n",
    "                ):\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif (\n",
    "                    c_min > np.finfo(np.float32).min\n",
    "                    and c_max < np.finfo(np.float32).max\n",
    "                ):\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    end_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    if verbose:\n",
    "        print(\n",
    "            \"Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)\".format(\n",
    "                end_mem, 100 * (start_mem - end_mem) / start_mem\n",
    "            )\n",
    "        )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    INPUT_DIR = \"/kaggle/input\" if on_kaggle() else \"input\"\n",
    "    INPUT_DIR = f\"{INPUT_DIR}/m5-forecasting-accuracy\"\n",
    "\n",
    "    print(\"Reading files...\")\n",
    "\n",
    "    calendar = pd.read_csv(f\"{INPUT_DIR}/calendar.csv\").pipe(reduce_mem_usage)\n",
    "    sell_prices = pd.read_csv(f\"{INPUT_DIR}/sell_prices.csv\").pipe(reduce_mem_usage)\n",
    "    sales_train_val = pd.read_csv(f\"{INPUT_DIR}/sales_train_validation.csv\").pipe(\n",
    "        reduce_mem_usage\n",
    "    )\n",
    "    submission = pd.read_csv(f\"{INPUT_DIR}/sample_submission.csv\").pipe(\n",
    "        reduce_mem_usage\n",
    "    )\n",
    "\n",
    "    print(\"Calendar shape:\", calendar.shape)\n",
    "    print(\"Sell prices shape:\", sell_prices.shape)\n",
    "    print(\"Sales train shape:\", sales_train_val.shape)\n",
    "    print(\"Submission shape:\", submission.shape)\n",
    "\n",
    "    return calendar, sell_prices, sales_train_val, submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "calendar, sell_prices, sales_train_val, submission = read_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "def melt(\n",
    "    sales_train_val, submission, nrows=55_000_000, verbose=True,\n",
    "):\n",
    "    # melt sales data, get it ready for training\n",
    "    id_columns = [\"id\", \"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\"]\n",
    "    sales_train_val = pd.melt(\n",
    "        sales_train_val, id_vars=id_columns, var_name=\"day\", value_name=\"demand\",\n",
    "    )\n",
    "\n",
    "    sales_train_val = reduce_mem_usage(sales_train_val)\n",
    "\n",
    "    if verbose:\n",
    "        display(sales_train_val)\n",
    "\n",
    "    # separate test dataframes.\n",
    "    test1 = submission[submission[\"id\"].str.contains(\"validation\")]\n",
    "    test2 = submission[submission[\"id\"].str.contains(\"evaluation\")]\n",
    "\n",
    "    if verbose:\n",
    "        display(test1, test2)\n",
    "\n",
    "    # change column names.\n",
    "    test1.columns = [\"id\"] + [f\"d_{x}\".format(x) for x in range(1914, 1914 + 28)]\n",
    "    test2.columns = [\"id\"] + [f\"d_{x}\".format(x) for x in range(1942, 1942 + 28)]\n",
    "\n",
    "    # get product table.\n",
    "    product = sales_train_val[id_columns].drop_duplicates()\n",
    "\n",
    "    # merge with product table\n",
    "    test2[\"id\"] = test2[\"id\"].str.replace(\"_evaluation\", \"_validation\")\n",
    "    test1 = test1.merge(product, how=\"left\", on=\"id\")\n",
    "    test2 = test2.merge(product, how=\"left\", on=\"id\")\n",
    "    test2[\"id\"] = test2[\"id\"].str.replace(\"_validation\", \"_evaluation\")\n",
    "\n",
    "    if verbose:\n",
    "        display(test1, test2)\n",
    "\n",
    "    test1 = pd.melt(test1, id_vars=id_columns, var_name=\"day\", value_name=\"demand\")\n",
    "    test2 = pd.melt(test2, id_vars=id_columns, var_name=\"day\", value_name=\"demand\")\n",
    "\n",
    "    if verbose:\n",
    "        display(test1, test2)\n",
    "\n",
    "    sales_train_val[\"part\"] = \"train\"\n",
    "    test1[\"part\"] = \"test1\"\n",
    "    test2[\"part\"] = \"test2\"\n",
    "\n",
    "    data = pd.concat([sales_train_val, test1, test2], axis=0)\n",
    "\n",
    "    if verbose:\n",
    "        display(data)\n",
    "\n",
    "    del sales_train_val, test1, test2\n",
    "\n",
    "    # get only a sample for fast training.\n",
    "    data = data.loc[nrows:]\n",
    "\n",
    "    # delete test2 for now.\n",
    "    data = data[data[\"part\"] != \"test2\"]\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def merge(data, calendar, sell_prices, verbose=True):\n",
    "    # drop some calendar features.\n",
    "    calendar = calendar.drop([\"weekday\", \"wday\", \"month\", \"year\"], axis=1)\n",
    "\n",
    "    # notebook crashes with the entire dataset.\n",
    "    data = pd.merge(data, calendar, how=\"left\", left_on=[\"day\"], right_on=[\"d\"])\n",
    "    data = data.drop([\"d\", \"day\"], axis=1)\n",
    "\n",
    "    if verbose:\n",
    "        display(data)\n",
    "\n",
    "    # get the sell price data (this feature should be very important).\n",
    "    data = data.merge(sell_prices, on=[\"store_id\", \"item_id\", \"wm_yr_wk\"], how=\"left\")\n",
    "\n",
    "    if verbose:\n",
    "        display(data)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "data = melt(sales_train_val, submission, nrows=27_500_000)\n",
    "data = merge(data, calendar, sell_prices)\n",
    "data = reduce_mem_usage(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "def encode_categoricals(df):\n",
    "    nan_features = [\"event_name_1\", \"event_type_1\", \"event_name_2\", \"event_type_2\"]\n",
    "    for feature in nan_features:\n",
    "        df[feature] = df[feature].fillna(\"MISSING\")\n",
    "\n",
    "    cat_cols = [\n",
    "        \"item_id\",\n",
    "        \"dept_id\",\n",
    "        \"cat_id\",\n",
    "        \"store_id\",\n",
    "        \"state_id\",\n",
    "        \"event_name_1\",\n",
    "        \"event_type_1\",\n",
    "        \"event_name_2\",\n",
    "        \"event_type_2\",\n",
    "    ]\n",
    "\n",
    "    for col in cat_cols:\n",
    "        encoder = LabelEncoder()\n",
    "        df[col] = encoder.fit_transform(df[col])\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_agg_features(df):\n",
    "    # rolling demand features.\n",
    "    for shift in [28, 29, 30]:\n",
    "        df[f\"shift_t{shift}\"] = df.groupby([\"id\"])[\"demand\"].transform(\n",
    "            lambda x: x.shift(shift)\n",
    "        )\n",
    "\n",
    "    for size in [7, 30]:\n",
    "        df[f\"rolling_std_t{size}\"] = df.groupby([\"id\"])[\"demand\"].transform(\n",
    "            lambda x: x.shift(28).rolling(size).std()\n",
    "        )\n",
    "\n",
    "    for size in [7, 30, 90, 180]:\n",
    "        df[f\"rolling_mean_t{size}\"] = df.groupby([\"id\"])[\"demand\"].transform(\n",
    "            lambda x: x.shift(28).rolling(size).mean()\n",
    "        )\n",
    "\n",
    "    df[\"rolling_skew_t30\"] = df.groupby([\"id\"])[\"demand\"].transform(\n",
    "        lambda x: x.shift(28).rolling(30).skew()\n",
    "    )\n",
    "    df[\"rolling_kurt_t30\"] = df.groupby([\"id\"])[\"demand\"].transform(\n",
    "        lambda x: x.shift(28).rolling(30).kurt()\n",
    "    )\n",
    "\n",
    "    # price features\n",
    "    df[\"shift_price_t1\"] = df.groupby([\"id\"])[\"sell_price\"].transform(\n",
    "        lambda x: x.shift(1)\n",
    "    )\n",
    "    df[\"price_change_t1\"] = (df[\"shift_price_t1\"] - df[\"sell_price\"]) / (\n",
    "        df[\"shift_price_t1\"]\n",
    "    )\n",
    "    df[\"rolling_price_max_t365\"] = df.groupby([\"id\"])[\"sell_price\"].transform(\n",
    "        lambda x: x.shift(1).rolling(365).max()\n",
    "    )\n",
    "    df[\"price_change_t365\"] = (df[\"rolling_price_max_t365\"] - df[\"sell_price\"]) / (\n",
    "        df[\"rolling_price_max_t365\"]\n",
    "    )\n",
    "\n",
    "    df[\"rolling_price_std_t7\"] = df.groupby([\"id\"])[\"sell_price\"].transform(\n",
    "        lambda x: x.rolling(7).std()\n",
    "    )\n",
    "    df[\"rolling_price_std_t30\"] = df.groupby([\"id\"])[\"sell_price\"].transform(\n",
    "        lambda x: x.rolling(30).std()\n",
    "    )\n",
    "    return df.drop([\"rolling_price_max_t365\", \"shift_price_t1\"], axis=1)\n",
    "\n",
    "\n",
    "def add_time_features(df):\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "    df[\"year\"] = df[\"date\"].dt.year\n",
    "    df[\"month\"] = df[\"date\"].dt.month\n",
    "    df[\"week\"] = df[\"date\"].dt.week\n",
    "    df[\"day\"] = df[\"date\"].dt.day\n",
    "    df[\"dayofweek\"] = df[\"date\"].dt.dayofweek\n",
    "    df[\"is_weekend\"] = df[\"dayofweek\"].isin([5, 6])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "data = encode_categoricals(data)\n",
    "data = add_agg_features(data)\n",
    "data = add_time_features(data)\n",
    "data = reduce_mem_usage(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "def train_lgb(bst_params, fit_params, X, y, cv):\n",
    "    models = []\n",
    "\n",
    "    for idx_fold, (idx_trn, idx_val) in enumerate(cv.split(X, y)):\n",
    "        print(f\"\\n---------- Fold: ({idx_fold + 1} / {cv.get_n_splits()}) ----------\\n\")\n",
    "\n",
    "        X_trn, X_val = X.iloc[idx_trn], X.iloc[idx_val]\n",
    "        y_trn, y_val = y.iloc[idx_trn], y.iloc[idx_val]\n",
    "        train_set = lgb.Dataset(X_trn, label=y_trn)\n",
    "        val_set = lgb.Dataset(X_val, label=y_val)\n",
    "\n",
    "        model = lgb.train(\n",
    "            bst_params,\n",
    "            train_set,\n",
    "            valid_sets=[train_set, val_set],\n",
    "            valid_names=[\"train\", \"valid\"],\n",
    "            **fit_params,\n",
    "        )\n",
    "        models.append(model)\n",
    "\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "features = [\n",
    "    \"item_id\",\n",
    "    \"dept_id\",\n",
    "    \"cat_id\",\n",
    "    \"store_id\",\n",
    "    \"state_id\",\n",
    "    \"event_name_1\",\n",
    "    \"event_type_1\",\n",
    "    \"event_name_2\",\n",
    "    \"event_type_2\",\n",
    "    \"snap_CA\",\n",
    "    \"snap_TX\",\n",
    "    \"snap_WI\",\n",
    "    \"sell_price\",\n",
    "    # aggregation features.\n",
    "    \"shift_t28\",\n",
    "    \"shift_t29\",\n",
    "    \"shift_t30\",\n",
    "    \"rolling_mean_t7\",\n",
    "    \"rolling_std_t7\",\n",
    "    \"rolling_mean_t30\",\n",
    "    \"rolling_mean_t90\",\n",
    "    \"rolling_mean_t180\",\n",
    "    \"rolling_std_t30\",\n",
    "    \"price_change_t1\",\n",
    "    \"price_change_t365\",\n",
    "    \"rolling_price_std_t7\",\n",
    "    \"rolling_price_std_t30\",\n",
    "    \"rolling_skew_t30\",\n",
    "    \"rolling_kurt_t30\",\n",
    "    # time features.\n",
    "    \"year\",\n",
    "    \"month\",\n",
    "    \"week\",\n",
    "    \"day\",\n",
    "    \"dayofweek\",\n",
    "]\n",
    "\n",
    "# prepare training and test data.\n",
    "# 2011-01-29 ~ 2016-04-24 : d_1    ~ d_1913\n",
    "# 2016-04-25 ~ 2016-05-22 : d_1914 ~ d_1941 (public)\n",
    "# 2016-05-23 ~ 2016-06-19 : d_1942 ~ d_1969 (private)\n",
    "\n",
    "mask1 = data[\"date\"] <= \"2016-03-27\"  # noqa\n",
    "mask2 = data[\"date\"] <= \"2016-04-24\"  # noqa\n",
    "X_train = data[mask2][features]\n",
    "y_train = data[mask2][\"demand\"]\n",
    "\n",
    "# X_val = data[~mask1 & mask2][features]\n",
    "# y_val = data[~mask1 & mask2][\"demand\"]\n",
    "\n",
    "X_test = data[~mask2][features]\n",
    "id_date = data[~mask2][[\"id\", \"date\"]]  # keep these two columns to use later.\n",
    "\n",
    "del data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "bst_params = {\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"metric\": \"rmse\",\n",
    "    \"objective\": \"regression\",\n",
    "    \"n_jobs\": -1,\n",
    "    \"seed\": 42,\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"bagging_fraction\": 0.75,\n",
    "    \"bagging_freq\": 10,\n",
    "    \"colsample_bytree\": 0.75,\n",
    "}\n",
    "\n",
    "fit_params = {\n",
    "    \"num_boost_round\": 100_000,\n",
    "    \"early_stopping_rounds\": 50,\n",
    "    \"verbose_eval\": 100,\n",
    "}\n",
    "\n",
    "cv = TimeSeriesSplit(n_splits=5)\n",
    "models = train_lgb(bst_params, fit_params, X_train, y_train, cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "imp_type = \"gain\"\n",
    "importances = np.zeros(X_train.shape[1])\n",
    "preds = np.zeros(X_test.shape[0])\n",
    "\n",
    "for model in models:\n",
    "    preds += model.predict(X_test)\n",
    "    importances += model.feature_importance(imp_type)\n",
    "\n",
    "# Take the average over folds.\n",
    "preds = preds / cv.get_n_splits()\n",
    "importances = importances / cv.get_n_splits()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/harupy/mlflow-extend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "from mlflow_extend import mlflow, plotting as mplt\n",
    "\n",
    "with mlflow.start_run():\n",
    "    mlflow.log_params_flatten(\n",
    "        {\n",
    "            \"bst\": bst_params,\n",
    "            \"fit\": fit_params,\n",
    "            \"cv\": {\"type\": str(TimeSeriesSplit), \"n_splits\": cv.get_n_splits()},\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "features = models[0].feature_name()\n",
    "_ = mplt.feature_importance(features, importances, imp_type, limit=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "def make_submission(test, submission):\n",
    "    preds = test[[\"id\", \"date\", \"demand\"]]\n",
    "    preds = pd.pivot(preds, index=\"id\", columns=\"date\", values=\"demand\").reset_index()\n",
    "    F_cols = [\"F\" + str(x + 1) for x in range(28)]\n",
    "    preds.columns = [\"id\"] + F_cols\n",
    "\n",
    "    evals = submission[submission[\"id\"].str.contains(\"evaluation\")]\n",
    "    vals = submission[[\"id\"]].merge(preds, how=\"inner\", on=\"id\")\n",
    "    final = pd.concat([vals, evals])\n",
    "\n",
    "    assert final[F_cols].isnull().sum().sum() == 0\n",
    "\n",
    "    final.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "make_submission(id_date.assign(demand=preds), submission)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "title,-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
